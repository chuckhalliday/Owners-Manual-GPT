{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f42dec9d",
   "metadata": {},
   "source": [
    "# PDF Manual Reader\n",
    "This is a simple chatbot which can look up a give pdf and answer your questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f74ec2",
   "metadata": {},
   "source": [
    "### Download and install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4ab6154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Using cached pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Collecting langchain\n",
      "  Using cached langchain-0.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting openai\n",
      "  Using cached openai-1.7.2-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Collecting PyYAML>=5.3 (from langchain)\n",
      "  Using cached PyYAML-6.0.1-cp311-cp311-macosx_10_9_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Using cached SQLAlchemy-2.0.25-cp311-cp311-macosx_10_9_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
      "  Using cached aiohttp-3.9.1-cp311-cp311-macosx_10_9_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
      "  Using cached dataclasses_json-0.6.3-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langchain-community<0.1,>=0.0.9 (from langchain)\n",
      "  Using cached langchain_community-0.0.12-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting langchain-core<0.2,>=0.1.7 (from langchain)\n",
      "  Using cached langchain_core-0.1.10-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting langsmith<0.1.0,>=0.0.77 (from langchain)\n",
      "  Using cached langsmith-0.0.80-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting numpy<2,>=1 (from langchain)\n",
      "  Using cached numpy-1.26.3-cp311-cp311-macosx_10_9_x86_64.whl.metadata (61 kB)\n",
      "Collecting pydantic<3,>=1 (from langchain)\n",
      "  Using cached pydantic-2.5.3-py3-none-any.whl.metadata (65 kB)\n",
      "Collecting requests<3,>=2 (from langchain)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain)\n",
      "  Using cached tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Using cached anyio-4.2.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Using cached httpx-0.26.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting sniffio (from openai)\n",
      "  Using cached sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting tqdm>4 (from openai)\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in ./.conda/lib/python3.11/site-packages (from openai) (4.9.0)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached multidict-6.0.4-cp311-cp311-macosx_10_9_x86_64.whl (29 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached yarl-1.9.4-cp311-cp311-macosx_10_9_x86_64.whl.metadata (31 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached frozenlist-1.4.1-cp311-cp311-macosx_10_9_x86_64.whl.metadata (12 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai)\n",
      "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Using cached marshmallow-3.20.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->openai)\n",
      "  Using cached certifi-2023.11.17-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Using cached httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
      "  Using cached jsonpointer-2.4-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in ./.conda/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1.7->langchain) (23.2)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1->langchain)\n",
      "  Using cached annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.14.6 (from pydantic<3,>=1->langchain)\n",
      "  Using cached pydantic_core-2.14.6-cp311-cp311-macosx_10_7_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2->langchain)\n",
      "  Using cached charset_normalizer-3.3.2-cp311-cp311-macosx_10_9_x86_64.whl.metadata (33 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchain)\n",
      "  Using cached urllib3-2.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Using cached greenlet-3.0.3-cp311-cp311-macosx_10_9_x86_64.whl\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Using cached langchain-0.1.0-py3-none-any.whl (797 kB)\n",
      "Using cached openai-1.7.2-py3-none-any.whl (212 kB)\n",
      "Using cached aiohttp-3.9.1-cp311-cp311-macosx_10_9_x86_64.whl (397 kB)\n",
      "Using cached anyio-4.2.0-py3-none-any.whl (85 kB)\n",
      "Using cached dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.26.0-py3-none-any.whl (75 kB)\n",
      "Using cached httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached langchain_community-0.0.12-py3-none-any.whl (1.6 MB)\n",
      "Using cached langchain_core-0.1.10-py3-none-any.whl (216 kB)\n",
      "Using cached langsmith-0.0.80-py3-none-any.whl (48 kB)\n",
      "Using cached numpy-1.26.3-cp311-cp311-macosx_10_9_x86_64.whl (20.6 MB)\n",
      "Using cached pydantic-2.5.3-py3-none-any.whl (381 kB)\n",
      "Using cached pydantic_core-2.14.6-cp311-cp311-macosx_10_7_x86_64.whl (1.9 MB)\n",
      "Using cached PyYAML-6.0.1-cp311-cp311-macosx_10_9_x86_64.whl (187 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached SQLAlchemy-2.0.25-cp311-cp311-macosx_10_9_x86_64.whl (2.1 MB)\n",
      "Using cached tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp311-cp311-macosx_10_9_x86_64.whl (121 kB)\n",
      "Using cached frozenlist-1.4.1-cp311-cp311-macosx_10_9_x86_64.whl (55 kB)\n",
      "Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Using cached jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Using cached marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
      "Using cached yarl-1.9.4-cp311-cp311-macosx_10_9_x86_64.whl (83 kB)\n",
      "Installing collected packages: urllib3, tqdm, tenacity, sniffio, PyYAML, python-dotenv, PyPDF2, pydantic-core, numpy, mypy-extensions, multidict, marshmallow, jsonpointer, idna, h11, greenlet, frozenlist, distro, charset-normalizer, certifi, attrs, annotated-types, yarl, typing-inspect, SQLAlchemy, requests, pydantic, jsonpatch, httpcore, anyio, aiosignal, langsmith, httpx, dataclasses-json, aiohttp, openai, langchain-core, langchain-community, langchain\n",
      "Successfully installed PyPDF2-3.0.1 PyYAML-6.0.1 SQLAlchemy-2.0.25 aiohttp-3.9.1 aiosignal-1.3.1 annotated-types-0.6.0 anyio-4.2.0 attrs-23.2.0 certifi-2023.11.17 charset-normalizer-3.3.2 dataclasses-json-0.6.3 distro-1.9.0 frozenlist-1.4.1 greenlet-3.0.3 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 idna-3.6 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.0 langchain-community-0.0.12 langchain-core-0.1.10 langsmith-0.0.80 marshmallow-3.20.2 multidict-6.0.4 mypy-extensions-1.0.0 numpy-1.26.3 openai-1.7.2 pydantic-2.5.3 pydantic-core-2.14.6 python-dotenv-1.0.0 requests-2.31.0 sniffio-1.3.0 tenacity-8.2.3 tqdm-4.66.1 typing-inspect-0.9.0 urllib3-2.1.0 yarl-1.9.4\n",
      "Requirement already satisfied: typing-extensions in ./.conda/lib/python3.11/site-packages (4.9.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2 langchain openai python-dotenv\n",
    "!pip install typing-extensions --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04fc9bf",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d18b1479",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader # for extracting text from pdf\n",
    "from langchain.text_splitter import CharacterTextSplitter # for splitting text in smaller snippets\n",
    "import os # for reading environment variables\n",
    "from dotenv import load_dotenv # for loading environment variables\n",
    "from openai import OpenAI # openai api\n",
    "import json # for storing snippets and embeddings in json\n",
    "from numpy import dot # for matching user questions with snippets.\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c56818",
   "metadata": {},
   "source": [
    "### Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b0ffd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTRACTED_TEXT_FILE_PATH = \"pdf.txt\" # text extracted from pdf\n",
    "EXTRACTED_JSON_PATH = \"extracted.json\" # snippets and embeddings\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY') # add your openai key in a .env file\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\" # embedding model used\n",
    "GPT_MODEL = \"gpt-3.5-turbo\" # gpt model used. alternatively you can use gpt-4 or other models.\n",
    "CHUNK_SIZE = 1000 # chunk size of snippets\n",
    "CHUNK_OVERLAP = 200 # check size to create overlap between snippets\n",
    "CONFIDENCE_SCORE = 0.75 # for filtering search results. [0,1] prefered: 0.75 or greater"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c47ddd",
   "metadata": {},
   "source": [
    "### Extract text from pdf\n",
    "Then save in - `pdf_text.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "915b0f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(file_path: str):\n",
    "    \n",
    "    reader = PdfReader(file_path)\n",
    "\n",
    "    number_of_pages = len(reader.pages)\n",
    "\n",
    "    pdf_text = \"\"\n",
    "\n",
    "    for i in range(number_of_pages):\n",
    "\n",
    "        page = reader.pages[i]\n",
    "\n",
    "        pdf_text += page.extract_text()\n",
    "        \n",
    "        # Add a newline after each page's text for readability\n",
    "        pdf_text += \"\\n\"\n",
    "    \n",
    "    # Specify the file path for the new text file\n",
    "    file_path = EXTRACTED_TEXT_FILE_PATH\n",
    "\n",
    "    # Write the content to the text file\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(pdf_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c978bb82",
   "metadata": {},
   "source": [
    "### Turn text into embeddings\n",
    "Allows the computer to perform mathematical operations and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27cc2695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(file_path: str):\n",
    "    \n",
    "    snippets = []\n",
    "    # Initialize a CharacterTextSplitter with specified settings\n",
    "    text_splitter = CharacterTextSplitter(separator=\"\\n\",\n",
    "                                         chunk_size=CHUNK_SIZE,\n",
    "                                         chunk_overlap=CHUNK_OVERLAP,\n",
    "                                         length_function=len)\n",
    "\n",
    "    # Read the content of the file specified by file_path\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            file_text = file.read()\n",
    "\n",
    "    # Split the text into snippets using the specified settings\n",
    "    snippets = text_splitter.split_text(file_text)\n",
    "\n",
    "    \n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    \n",
    "    # Request embeddings for the snippets using the specified model\n",
    "    response = client.embeddings.create(input=snippets,model=EMBEDDING_MODEL)\n",
    "    \n",
    "    # Extract embeddings from the API response\n",
    "    embedding_list = [response_object.embedding for response_object in response.data]\n",
    "\n",
    "    # Create a JSON object containing embeddings and snippets\n",
    "    embedding_json = {\n",
    "        'embeddings': embedding_list,\n",
    "        'snippets': snippets\n",
    "    }\n",
    "    \n",
    "    # Convert the JSON object to a formatted JSON string\n",
    "    json_object = json.dumps(embedding_json, indent=4)\n",
    "\n",
    "    # Write the JSON string to a file specified by EXTRACTED_JSON_PATH\n",
    "    with open(EXTRACTED_JSON_PATH, 'w', encoding=\"utf-8\") as file:\n",
    "        file.write(json_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c821fc9f",
   "metadata": {},
   "source": [
    "### Read Embedding JSON file\n",
    "Reads `extracted.json` and prepared embedding for the advisor bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc35e61f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_embeddings():\n",
    "    \n",
    "    # Open the JSON file containing embeddings and snippets\n",
    "    with open(EXTRACTED_JSON_PATH,'r') as file:\n",
    "        # Load the JSON data into a Python dictionary\n",
    "        embedding_json = json.load(file)\n",
    "        \n",
    "    # Return the embeddings and snippets from the loaded JSON\n",
    "    return embedding_json['embeddings'], embedding_json['snippets']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4dcbb1",
   "metadata": {},
   "source": [
    "### Create Embedding from User's Question\n",
    "Output of this function is used to find the right embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "669f42a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_question_embedding_creator(question):\n",
    "    \n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    \n",
    "    # Request embedding for the provided question using the specified model\n",
    "    response = client.embeddings.create(input=question,model=EMBEDDING_MODEL)\n",
    "    \n",
    "    # Extract and return the embedding from the API response\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d016c0f",
   "metadata": {},
   "source": [
    "### Answer user's question\n",
    "The main event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93a1742d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_users_question(user_question):\n",
    "    \n",
    "    try:\n",
    "        # Create an embedding for the user's question\n",
    "        user_question_embedding = user_question_embedding_creator(user_question)\n",
    "    except Exception as e:\n",
    "        # Handle any exception that occurred while using Embedding API.\n",
    "        return f\"An error occurred while creating embedding: {str(e)}\"\n",
    "        \n",
    "    \n",
    "    # Calculate cosine similarities between the user's question embedding and the document embeddings\n",
    "    cosine_similarities = []\n",
    "    for embedding in embeddings:\n",
    "        cosine_similarities.append(dot(user_question_embedding,embedding))\n",
    "\n",
    "    # Pair snippets with their respective cosine similarities and sort them by similarity\n",
    "    scored_snippets = zip(snippets, cosine_similarities)\n",
    "    sorted_snippets = sorted(scored_snippets, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Filter snippets based on a confidence score and select the top 5 results\n",
    "    formatted_top_results = [snipps for snipps, _score in sorted_snippets if _score > CONFIDENCE_SCORE]\n",
    "    if len(formatted_top_results) > 5:\n",
    "        formatted_top_results = formatted_top_results[:5]\n",
    "        \n",
    "    # Create the chatbot system using pdf_description provided by the user.\n",
    "    chatbot_system = f\"\"\"You are provided with SEARCH RESULTS from a pdf. You need to generate answer to the user's question based on the given SEARCH RESULTS. SEARCH RESULTS as a python list. SEARCH RESULTS and USER's QUESTION are delimited by ``` \\n If there is no information available, or question is irrelevent respond with - \"Sorry! I can't help you.\" \"\"\"\n",
    "    \n",
    "    # Create the prompt using results and user's question.\n",
    "    prompt = f\"\"\"\\\n",
    "    SEARCH RESULTS:\n",
    "    ```\n",
    "    {formatted_top_results}\n",
    "    ```\n",
    "    USER'S QUESTION:\n",
    "    ```\n",
    "    {user_question}\n",
    "    ```\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare the chat conversation and use GPT model for generating a response\n",
    "    messages = [{'role':'system', 'content':chatbot_system},\n",
    "                {'role':'user', 'content':prompt}]\n",
    "    \n",
    "    try:\n",
    "        client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "        completion = client.chat.completions.create(model=GPT_MODEL,\n",
    "                                             messages=messages,\n",
    "                                             temperature=0,\n",
    "                                             stream=False)\n",
    "    except Exception as e:\n",
    "        # Handle exception while communicating with ChatCompletion API\n",
    "        return f\"An error occurred with chatbot: {str(e)}\"\n",
    "        \n",
    "    # Return the chatbot response.\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b36eea",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145b0cf4",
   "metadata": {},
   "source": [
    "## Start Here\n",
    "After initiating environment, specify the path to pdf below and run all the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9925b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_FILE_PATH = \"2019Forester.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23899f6",
   "metadata": {},
   "source": [
    "**Converting pdf to text file.**\n",
    "> âš ï¸ **ONLY RUN ONCE.**\n",
    "> You only need to extract text once.\n",
    "\n",
    "Add `#` to the beginning of this function after extracting text from the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51cc31ef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# extract_text_from_pdf(PDF_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf74411",
   "metadata": {},
   "source": [
    "**Creating embeddings from text file.**\n",
    "\n",
    "> âš ï¸ **ONLY RUN ONCE PER PDF**\n",
    "> This is a BILLED FUNCTION.\n",
    "\n",
    "`create_embeddings` function is billed, it uses OpenAI's APIs to create embeddings, use it only when required. Comment it after creating embeddings from the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2da4152",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create_embeddings(EXTRACTED_TEXT_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee02280",
   "metadata": {},
   "source": [
    "**Prepare Embeddings**\n",
    "\n",
    "This reads the embeddings from the json file and stores them for chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64eadb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, snippets = get_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827370bb",
   "metadata": {},
   "source": [
    "## Chatbot\n",
    "\n",
    "**To exit leave user input blank and hit enter.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f5c6a8d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘¤USER: Where is the cabin filter?\n",
      "----------------------\n",
      "ðŸ¤– ADVISOR BOT:\n",
      "The cabin air filter is located behind the glove box. To replace the cabin air filter, you need to follow these steps:\n",
      "1. Remove the glove box by opening it and removing the damper shaft.\n",
      "2. Push in the stoppers on both sides of the glove box and pull down the glove box as far as it will go.\n",
      "3. Pull out the glove box horizontally and remove the hinge portion.\n",
      "4. Remove the cabin air filter by pushing in the four stoppers to unlock and slowly pulling it out from the housing.\n",
      "5. Install a new cabin air filter with the arrow mark pointing up.\n",
      "6. Reinstall the glove box and connect the damper shaft.\n",
      "7. Close the glove box.\n",
      "\n",
      "Please note that this information is specific to the \"åŒ—ç±³Model \"A8240BE-E\" and may vary for other models.\n",
      "----------------------\n",
      "ðŸ‘¤USER: \n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "# Start an infinite loop, allowing the user to ask questions\n",
    "while True:\n",
    "\n",
    "    user_question = input(\"\")\n",
    "    \n",
    "    # Prompt the user to input a question\n",
    "    print(\"ðŸ‘¤USER: \" + user_question)\n",
    "    \n",
    "    # Print a separator for readability\n",
    "    print(\"----------------------\")\n",
    "    \n",
    "    # Check if the user entered an empty question\n",
    "    if user_question ==\"\":\n",
    "        \n",
    "        # If the user entered an empty question, exit the loop\n",
    "        break\n",
    "    else:\n",
    "        \n",
    "        # If the user entered a question, proceed to generate a response\n",
    "        print(\"ðŸ¤– ADVISOR BOT:\")\n",
    "        \n",
    "        # Call the function to generate an answer based on the user's question\n",
    "        # and print the bot's response\n",
    "        print(answer_users_question(user_question=user_question))\n",
    "        \n",
    "        # Print a separator for readability\n",
    "        print(\"----------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
